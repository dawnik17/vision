{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54cc780c-ef0b-48a2-b677-ce3d1a338b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d641fd8-e343-4ced-af7b-fa53a4a7e94f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# this ensures that the current MacOS version is at least 12.3+\n",
    "print(torch.backends.mps.is_available())\n",
    "# this ensures that the current current PyTorch installation was built with MPS activated.\n",
    "print(torch.backends.mps.is_built())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93622f9c-9122-4bd5-a11f-e0ada797f290",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "74369673-25ec-4c8c-a4bc-7c83b992ee62",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, dimension, nheads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert dimension % nheads == 0\n",
    "        self.nheads = nheads\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.query = nn.Linear(dimension, dimension)\n",
    "        self.key = nn.Linear(dimension, dimension)\n",
    "        self.value = nn.Linear(dimension, dimension)\n",
    "        self.out = nn.Linear(dimension, dimension)\n",
    "\n",
    "    def forward(self, query, key, value, mask):\n",
    "        batch_size = query.shape[0]\n",
    "        seq_len = query.shape[1]\n",
    "        dimension = query.shape[2]\n",
    "\n",
    "        query = self.query(query)\n",
    "        key = self.key(key)\n",
    "        value = self.value(value)\n",
    "\n",
    "        # head split\n",
    "        query = query.view(\n",
    "            batch_size, seq_len, self.nheads, dimension // self.nheads\n",
    "        ).transpose(2, 1)\n",
    "        key = key.view(\n",
    "            batch_size, seq_len, self.nheads, dimension // self.nheads\n",
    "        ).transpose(2, 1)\n",
    "        value = value.view(\n",
    "            batch_size, seq_len, self.nheads, dimension // self.nheads\n",
    "        ).transpose(2, 1)\n",
    "\n",
    "        # attention\n",
    "        attn = self.attention(query, key, mask)\n",
    "        output = torch.matmul(attn, value)\n",
    "        output = output.transpose(2, 1).reshape(batch_size, seq_len, dimension)\n",
    "        return self.out(output)\n",
    "\n",
    "    def attention(self, query, key, mask):\n",
    "        qk_t = torch.matmul(query, key.transpose(3, 2))\n",
    "        qk_t = qk_t / math.sqrt(query.shape[-1])\n",
    "\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1)\n",
    "            qk_t = qk_t.fill_mask(mask == 0, 1e-9)\n",
    "\n",
    "        qk_t = self.dropout(qk_t)\n",
    "        return F.softmax(qk_t, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "e9deaac6-359f-4cf7-8b26-5f1fc5395e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, dimension):\n",
    "        super().__init__()\n",
    "        self.mean_reverse = nn.Parameter(torch.zeros(dimension))\n",
    "        self.std_reverse = nn.Parameter(torch.ones(dimension))\n",
    "        self.delta = 1e-6\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        mean = torch.mean(x, dim=-1, keepdim=True)\n",
    "        std = torch.std(x, dim=-1, keepdim=True) + self.delta\n",
    "        return (self.std_reverse / std) * (x - mean + self.mean_reverse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "874875c7-28e2-4e66-a380-4ca351904952",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualConnection(nn.Module):\n",
    "    def __init__(self, dimension, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.norm = LayerNorm(dimension=dimension)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, layer):\n",
    "        return x + self.dropout(layer(self.norm(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "06a78c40-66d6-4260-b053-3519b3ada093",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNet(nn.Module):\n",
    "    def __init__(self, dimension, hidden_dim=1024, dropout=0.1, activation=\"relu\"):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dimension, hidden_dim),\n",
    "            nn.ReLU() if activation == \"relu\" else nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, dimension),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "7885db6f-8324-452f-be8b-4abdf8fc775d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(\n",
    "        self, dimension, nheads, hidden_dimension=1024, activation=\"relu\", dropout=0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.mha = MultiHeadAttention(\n",
    "            dimension=dimension, nheads=nheads, dropout=dropout\n",
    "        )\n",
    "        self.ffnn = FeedForwardNet(\n",
    "            dimension=dimension,\n",
    "            hidden_dim=hidden_dimension,\n",
    "            dropout=dropout,\n",
    "            activation=activation,\n",
    "        )\n",
    "        self.resconn1 = ResidualConnection(dimension=dimension, dropout=dropout)\n",
    "        self.resconn2 = ResidualConnection(dimension=dimension, dropout=dropout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x = self.resconn1(x, lambda x: self.mha(query=x, key=x, value=x, mask=mask))\n",
    "        return self.resconn2(x, self.ffnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "056c245c-f6d2-4e62-ac87-785bb227e52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy as copy\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size,\n",
    "        max_seq_len,\n",
    "        nlayers,\n",
    "        dimension,\n",
    "        nheads,\n",
    "        hidden_dimension,\n",
    "        activation,\n",
    "        dropout,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, dimension)\n",
    "        self.pemb = nn.Embedding(max_seq_len, dimension)\n",
    "\n",
    "        self.encoder_layers = nn.ModuleList(\n",
    "            [\n",
    "                copy(\n",
    "                    EncoderLayer(\n",
    "                        dimension=dimension,\n",
    "                        nheads=nheads,\n",
    "                        hidden_dimension=hidden_dimension,\n",
    "                        activation=activation,\n",
    "                        dropout=dropout,\n",
    "                    )\n",
    "                )\n",
    "                for _ in range(nlayers)\n",
    "            ]\n",
    "        )\n",
    "        self.norm = LayerNorm(dimension=dimension)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x = self.emb(x)\n",
    "        x = x + self.pemb(torch.arange(x.shape[1]))\n",
    "\n",
    "        for layer in self.encoder_layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "d0338f69-a2f6-42e2-a298-2a34ac7ddd19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4, 256])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample run\n",
    "vocab_size = 1000\n",
    "encoder = Encoder(\n",
    "    vocab_size=1000,\n",
    "    max_seq_len=100,\n",
    "    nlayers=2,\n",
    "    dimension=256,\n",
    "    nheads=8,\n",
    "    hidden_dimension=512,\n",
    "    activation=\"gelu\",\n",
    "    dropout=0.1,\n",
    ")\n",
    "encoder(torch.randint(low=0, high=vocab_size, size=(3, 4)), None).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "0da5da4f-00f8-45ba-9238-5ee768599ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(\n",
    "        self, dimension, nheads, hidden_dimension=1024, activation=\"relu\", dropout=0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.ffnn = FeedForwardNet(\n",
    "            dimension=dimension,\n",
    "            hidden_dim=hidden_dimension,\n",
    "            dropout=dropout,\n",
    "            activation=activation,\n",
    "        )\n",
    "        self.resconn = nn.ModuleList(\n",
    "            [\n",
    "                copy(ResidualConnection(dimension=dimension, dropout=dropout))\n",
    "                for _ in range(3)\n",
    "            ]\n",
    "        )\n",
    "        self.mha = nn.ModuleList(\n",
    "            [\n",
    "                copy(\n",
    "                    MultiHeadAttention(\n",
    "                        dimension=dimension, nheads=nheads, dropout=dropout\n",
    "                    )\n",
    "                )\n",
    "                for _ in range(2)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def forward(self, x, encoder_output, encmask, decmask):\n",
    "        # self attention\n",
    "        x = self.resconn[0](\n",
    "            x, lambda x: self.mha[0](query=x, key=x, value=x, mask=decmask)\n",
    "        )\n",
    "\n",
    "        # cross attention\n",
    "        x = self.resconn[1](\n",
    "            x,\n",
    "            lambda x: self.mha[1](\n",
    "                query=x, key=encoder_output, value=encoder_output, mask=encmask\n",
    "            ),\n",
    "        )\n",
    "        return self.resconn[2](x, self.ffnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "83edfd88-2ba8-4e2c-bda8-e2a4e4615e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size,\n",
    "        max_seq_len,\n",
    "        nlayers,\n",
    "        dimension,\n",
    "        nheads,\n",
    "        hidden_dimension,\n",
    "        activation,\n",
    "        dropout,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, dimension)\n",
    "        self.pemb = nn.Embedding(max_seq_len, dimension)\n",
    "\n",
    "        self.decoder_layers = nn.ModuleList(\n",
    "            [\n",
    "                copy(\n",
    "                    DecoderLayer(\n",
    "                        dimension=dimension,\n",
    "                        nheads=nheads,\n",
    "                        hidden_dimension=hidden_dimension,\n",
    "                        activation=activation,\n",
    "                        dropout=dropout,\n",
    "                    )\n",
    "                )\n",
    "                for _ in range(nlayers)\n",
    "            ]\n",
    "        )\n",
    "        self.norm = LayerNorm(dimension=dimension)\n",
    "\n",
    "    def forward(self, x, encoder_output, encmask, decmask):\n",
    "        x = self.emb(x)\n",
    "        x = x + self.pemb(torch.arange(x.shape[1]))\n",
    "\n",
    "        for layer in self.decoder_layers:\n",
    "            x = layer(x, encoder_output, encmask, decmask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "53b60abd-9111-46d3-bcbf-0f89da3ad73a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4, 256])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample\n",
    "dimension = 256\n",
    "vocab_size = 1000\n",
    "batch = 3\n",
    "seqlen = 4\n",
    "x = torch.randint(low=0, high=vocab_size, size=(batch, seqlen))\n",
    "\n",
    "decoder = Decoder(\n",
    "    vocab_size=1000,\n",
    "    max_seq_len=100,\n",
    "    nlayers=2,\n",
    "    dimension=dimension,\n",
    "    nheads=8,\n",
    "    hidden_dimension=512,\n",
    "    activation=\"gelu\",\n",
    "    dropout=0.1,\n",
    ")\n",
    "decoder(\n",
    "    x=x,\n",
    "    encoder_output=torch.rand((batch, seqlen, dimension)),\n",
    "    encmask=None,\n",
    "    decmask=None,\n",
    ").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3bd596-05c0-4f3a-a3a9-9789b11c5026",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder_vocab_size,\n",
    "        decoder_vocab_size,\n",
    "        encoder_max_seq_len,\n",
    "        decoder_max_seq_len,\n",
    "        encoder_nlayers,\n",
    "        decoder_nlayers,\n",
    "        dimension,\n",
    "        nheads,\n",
    "        hidden_dimension,\n",
    "        activation,\n",
    "        dropout,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(\n",
    "            vocab_size=encoder_vocab_size,\n",
    "            max_seq_len=encoder_max_seq_len,\n",
    "            nlayers=encoder_nlayers,\n",
    "            dimension=dimension,\n",
    "            nheads=nheads,\n",
    "            hidden_dimension=hidden_dimension,\n",
    "            activation=activation,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "\n",
    "        self.decoder = Decoder(\n",
    "            vocab_size=decoder_vocab_size,\n",
    "            max_seq_len=decoder_max_seq_len,\n",
    "            nlayers=decoder_nlayers,\n",
    "            dimension=dimension,\n",
    "            nheads=nheads,\n",
    "            hidden_dimension=hidden_dimension,\n",
    "            activation=activation,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "        self.ffnn = nn.Linear(dimension, decoder_vocab_size)\n",
    "\n",
    "    def forward(self, encoder_input, decoder_input, encmask, decmask):\n",
    "        encoder_output = self.encoder(encoder_input, encmask)\n",
    "        decoder_output = self.decoder(decoder_input, decmask)\n",
    "        output = self.ffnn(decoder_output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "227c14db-b0e0-42f9-9633-e83e1c45536a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 heads 256 dimension in MultiHeadedAttention\n",
      "8 heads 256 dimension in MultiHeadedAttention\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Encoder(\n",
       "  (enclays): ModuleList(\n",
       "    (0-1): 2 x EncoderLayer(\n",
       "      (attn): MultiHeadedAttention(\n",
       "        (wq): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (wk): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (wv): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (out): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ffnn): FeedForwardNet(\n",
       "        (l): Linear(in_features=256, out_features=2048, bias=True)\n",
       "        (out): Linear(in_features=2048, out_features=256, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (resconn1): ResidualConnection(\n",
       "        (norm): LayerNorm()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (resconn2): ResidualConnection(\n",
       "        (norm): LayerNorm()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm): LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm()\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Encoder(2, 8, 256, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc621e18-e2d6-4659-9025-811642e9ccfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def anagram(s):\n",
    "    if len(s) % 2 != 0:\n",
    "        return -1\n",
    "        \n",
    "    half = len(s) // 2\n",
    "    \n",
    "    first = s[:half]\n",
    "    second = s[half:]\n",
    "    \n",
    "    counter = dict()\n",
    "    \n",
    "    for char in first:\n",
    "        if char not in counter:\n",
    "            counter[char] = 1\n",
    "            \n",
    "        else:\n",
    "            counter[char] += 1\n",
    "            \n",
    "    correction = 0\n",
    "    \n",
    "    for char in second:\n",
    "        if char in counter:\n",
    "            counter[char] -= 1\n",
    "            \n",
    "            if counter[char] == 0:\n",
    "                del counter[char]\n",
    "                 \n",
    "        else:\n",
    "            correction+=1\n",
    "\n",
    "    return correction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "2e07677c-f99e-40ce-b192-7f17f334cd6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('debb', 'debb')"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://www.hackerrank.com/contests/one-last-timeagain/challenges/problem-1-1-7\n",
    "def twist_string(s, twist):\n",
    "    f = lambda char, nroll: chr(ord(\"a\") + ((ord(char) - ord(\"a\")) + nroll) % 26)\n",
    "    s = list(s)\n",
    "    \n",
    "    for idx in twist:\n",
    "        s[:idx] = [f(char, 1) for char in s[:idx]]\n",
    "    \n",
    "    return \"\".join(s)\n",
    "\n",
    "\n",
    "def twist_string(s, twist):\n",
    "    f = lambda char, nroll: chr(ord(\"a\") + ((ord(char) - ord(\"a\")) + nroll) % 26)\n",
    "\n",
    "    inplace_twist = [0] * len(s)\n",
    "\n",
    "    for idx in twist:\n",
    "        inplace_twist[idx-1] += 1\n",
    "\n",
    "    cumulative_twist = 0\n",
    "    output = []\n",
    "    \n",
    "    for i in range(len(s) - 1, -1, -1):\n",
    "        cumulative_twist += inplace_twist[i]\n",
    "        output.append(f(s[i], cumulative_twist))\n",
    "\n",
    "    return \"\".join(output)[::-1]\n",
    "\n",
    "\n",
    "def findRollOut(s, roll):\n",
    "    def solve(st , A):\n",
    "        ch = ord(st) - ord('a')\n",
    "        return chr(ord('a') + (ch + A) % 26)\n",
    "\n",
    "    n = len(s)\n",
    "    rolls = [0] * n\n",
    "    \n",
    "    for i in roll:\n",
    "        rolls[i - 1] += 1\n",
    "\n",
    "    ans = 0\n",
    "    res = []\n",
    "    \n",
    "    for i in range(n - 1 , -1 , -1):\n",
    "       ans += rolls[i] \n",
    "       res.append(solve(s[i] , ans))\n",
    "     \n",
    "    return ''.join(res[::-1])\n",
    "\n",
    "s = \"zcza\"\n",
    "twist = [1,1,3,4]\n",
    "findRollOut(s, twist), twist_string(s, twist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "922a6589-4580-49e5-b8f1-d8c02a90b5cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1, 3, 2, 4], [1, 3, 2, 4])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://takeitoutamber.medium.com/hackerrank-coding-interview-1-queue-at-atm-b2e0e1859d3d\n",
    "\"\"\"\n",
    "from math import ceil\n",
    "\n",
    "\n",
    "f = lambda x, k: (x - 1 + k) // k\n",
    "g = lambda x, k: ceil(x / k)\n",
    "\"\"\"\n",
    "\n",
    "def getFinalOrder_(k, amount):\n",
    "    assert 1 <= len(amount) <= 10 ** 5\n",
    "    assert all(1 <= x <= 10 ** 9 for x in amount)\n",
    "    assert 1 <= k <= 10 ** 6\n",
    " \n",
    "    arr = []\n",
    "    for i in range(len(amount)):\n",
    "        arr.append([(amount[i] + k - 1) // k, i + 1])\n",
    "\n",
    "    arr.sort()\n",
    "    arr2 = [x[1] for x in arr]\n",
    "    return arr2\n",
    "\n",
    "\n",
    "def getFinalOrder(k, amount):\n",
    "    arr = [[ceil(a / k), idx] for idx, a in enumerate(amount)]\n",
    "    return [a[1] + 1 for a in sorted(arr, key=lambda x: x[0], reverse=False)]\n",
    "\n",
    "\n",
    "k = 2\n",
    "array = [3, 6, 4, 5]\n",
    "getFinalOrder_(k, array), getFinalOrder(k, array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f68380-055d-47ad-a8bc-03c999565432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.quora.com/If-it-takes-852-digits-to-number-the-pages-of-a-book-How-many-pages-are-there\n",
    "\n",
    "\"\"\"\n",
    "1-9 = 9 digits\n",
    "10-99 = 90 * 2 digits\n",
    "100-199 = 100 * 3 digits\n",
    "100-999 = 900 * 3 digits\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "df457743-a342-4ddd-8a79-2f1003f5baf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Something is happening before the function is called.\n",
      "Hello!\n",
      "Something is happening after the function is called.\n"
     ]
    }
   ],
   "source": [
    "def my_decorator(func):\n",
    "    def wrapper():\n",
    "        print(\"Something is happening before the function is called.\")\n",
    "        func()\n",
    "        print(\"Something is happening after the function is called.\")\n",
    "    return wrapper\n",
    "\n",
    "@my_decorator\n",
    "def say_hello():\n",
    "    print(\"Hello!\")\n",
    "\n",
    "say_hello()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "886d7730-9c72-4754-ac09-04976cfd5fd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling add with arguments (3, 5) and keyword arguments {}.\n",
      "add returned 8.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import functools\n",
    "\n",
    "def log_function_call(func):\n",
    "    @functools.wraps(func)  # Preserves the original function's metadata\n",
    "    def wrapper(*args, **kwargs):\n",
    "        result = func(*args, **kwargs)\n",
    "        print(f\"Calling {func.__name__} with arguments {args} and keyword arguments {kwargs}.\")\n",
    "        print(f\"{func.__name__} returned {result}.\")\n",
    "        return result\n",
    "    return wrapper\n",
    "\n",
    "@log_function_call\n",
    "def add(a, b):\n",
    "    return a + b\n",
    "\n",
    "# Usage:\n",
    "add(3, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "d887db9b-6717-4563-a978-2ad43780494f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating result for 5...\n",
      "10\n",
      "returning from cache..\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "import functools\n",
    "\n",
    "def memoize(func):\n",
    "    cache = {}\n",
    "\n",
    "    @functools.wraps(func)\n",
    "    def wrapper(*args):\n",
    "        if args in cache:\n",
    "            print(\"returning from cache..\")\n",
    "            return cache[args]\n",
    "        else:\n",
    "            result = func(*args)\n",
    "            cache[args] = result\n",
    "            return result\n",
    "    return wrapper\n",
    "\n",
    "@memoize\n",
    "def expensive_calculation(n):\n",
    "    print(f\"Calculating result for {n}...\")\n",
    "    return n * 2\n",
    "\n",
    "# Usage:\n",
    "print(expensive_calculation(5))  # Calculation is performed\n",
    "print(expensive_calculation(5))  # Result is retrieved from the cache, no recalculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "0e1dc62e-750c-4288-8357-654b9933dce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "returning from cache..\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "102"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expensive_calculation(51)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "8d3249ea-b910-44a2-840a-816c56bc5f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "@memoize\n",
    "def func(n):\n",
    "    print(f\"Calculating result for {n}...\")\n",
    "    return n // 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "8817aa54-f9d8-4eb8-8c04-a9840cee49b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "returning from cache..\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "func(51)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "18b9cd82-86c6-4151-9e4b-fcf632b009fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "# code for testing decorator chaining\n",
    "def decor1(func):\n",
    "\tdef inner():\n",
    "\t\tx = func()\n",
    "\t\treturn x * x\n",
    "\treturn inner\n",
    "\n",
    "def decor(func):\n",
    "\tdef inner():\n",
    "\t\tx = func()\n",
    "\t\treturn 2 * x\n",
    "\treturn inner\n",
    "\n",
    "@decor1\n",
    "@decor\n",
    "def num():\n",
    "\treturn 10\n",
    "\n",
    "@decor\n",
    "@decor1\n",
    "def num2():\n",
    "\treturn 10\n",
    "\n",
    "print(num())\n",
    "print(num2())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2533851-250b-48d2-b7c9-e2cd83e19ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1. Normalisation Types (tick)\n",
    "2. Attention (tick)\n",
    "3. Transformer Architecture (tick)\n",
    "4. Different types of activations (tick)\n",
    "5. Docker vs Virtual Environment (tick) \n",
    "6. API \n",
    "7. Chatbot \n",
    "8. How to reduce/stop hallucinations \n",
    "9. Lambda Function / Decorator / Filter / Map\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "d8da0780-cb83-41a2-a265-9f611ce6a6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lambda prefix, x: [[prefix + str(i)] for i in range(x)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "500783c7-8143-4a63-86df-db4d6125c49c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['q0'], ['q1'], ['q2']] [['k0'], ['k1'], ['k2']] [['v0'], ['v1'], ['v2']]\n"
     ]
    }
   ],
   "source": [
    "query = f(\"q\", 3)\n",
    "key = f(\"k\", 3)\n",
    "value = f(\"v\", 3)\n",
    "\n",
    "query, key, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f5be47-5f25-40ef-a28b-7e322f0d257b",
   "metadata": {},
   "outputs": [],
   "source": [
    "[q0k0, q0k1, 0]\n",
    "[q1k0, q1k1, 0]   / math.sqrt(dim)\n",
    "[q2k0, q2k1, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5dc499-c62b-4a90-90dc-61034c6e6bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = residual + normalisation\n",
    "\n",
    "Embedding + PE \n",
    "-> Encoder(f(mh.attention) + f(FFNN)) \n",
    "-> Decoder(f(self attention) f(cross attention) + f(FFNN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fdb187a-45d4-4c3a-bfc3-491418cadfe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Mehek mah darlin\n",
    "\n",
    "q1, q2, q3\n",
    "k1, k2, k3, k4\n",
    "v1, v2, v3, v4\n",
    "\n",
    "softmax((q1k1, q1k2, q1k3, q1k4) / sqrt(dim)))\n",
    "q1k1.v1 + q1k2.v2 + q1k3.v3 + q1k4.v4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
